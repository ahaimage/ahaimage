# 啊哈，图像！

![image](https://user-images.githubusercontent.com/101693157/158524594-40e4376b-af71-4235-a7b1-ea72ddd9c576.png)
> 矩阵计算

## 技术原理

### [SVG 图像入门教程](https://www.ruanyifeng.com/blog/2018/08/svg.html) 

![image](https://user-images.githubusercontent.com/101693157/158824737-6329411e-3c30-4edc-99fe-7e06f084e6c0.png)

SVG 是一种基于 XML 语法的图像格式，全称是可缩放矢量图（Scalable Vector Graphics）。`其他图像格式都是基于像素处理的，SVG 则是属于对图像的形状描述，所以它本质上是文本文件，体积较小，且不管放大多少倍都不会失真。`

SVG 文件可以直接插入网页，成为 DOM 的一部分，然后用 JavaScript 和 CSS 进行操作。

SVG 代码也可以写在一个独立文件中，然后用<img>、<object>、<embed>、<iframe>等标签插入网页。

### [相似图片搜索的原理](https://www.ruanyifeng.com/blog/2011/07/principle_of_similar_image_search.html) 

2011年6月，Google把"相似图片搜索"正式放上了首页。
你可以用一张图片，搜索互联网上所有与它相似的图片。点击搜索框中照相机的图标。

![image](https://user-images.githubusercontent.com/101693157/158818343-234ae8e7-113b-46e3-9c4f-88434fdff1d5.png)

一个对话框会出现。

你输入网片的网址，或者直接上传图片，Google就会找出与其相似的图片。下面这张图片是美国女演员Alyson Hannigan。
上传后，Google返回如下结果：
类似的"相似图片搜索引擎"还有不少，TinEye甚至可以找出照片的拍摄背景。

这种技术的原理是什么？计算机怎么知道两张图片相似呢？

根据[Neal Krawetz](https://www.hackerfactor.com/blog/index.php?/archives/432-Looks-Like-It.html)博士的解释，原理非常简单易懂。我们可以用一个快速算法，就达到基本的效果。

> 这里的关键技术叫做"感知哈希算法"（Perceptual hash algorithm），它的作用是对每张图片生成一个"指纹"（fingerprint）字符串，然后比较不同图片的指纹。结果越接近，就说明图片越相似。

下面是一个最简单的实现：

第一步，缩小尺寸。
将图片缩小到8x8的尺寸，总共64个像素。这一步的作用是去除图片的细节，只保留结构、明暗等基本信息，摒弃不同尺寸、比例带来的图片差异。
第二步，简化色彩。
将缩小后的图片，转为64级灰度。也就是说，所有像素点总共只有64种颜色。
第三步，计算平均值。
计算所有64个像素的灰度平均值。
第四步，比较像素的灰度。
将每个像素的灰度，与平均值进行比较。大于或等于平均值，记为1；小于平均值，记为0。
第五步，计算哈希值。
将上一步的比较结果，组合在一起，就构成了一个64位的整数，这就是这张图片的指纹。组合的次序并不重要，只要保证所有图片都采用同样次序就行了。

得到指纹以后，就可以对比不同的图片，看看64位中有多少位是不一样的。在理论上，这等同于计算"汉明距离"（Hamming distance）。如果不相同的数据位不超过5，就说明两张图片很相似；如果大于10，就说明这是两张不同的图片。
具体的代码实现，可以参见Wote用python语言写的imgHash.py。代码很短，只有53行。使用的时候，第一个参数是基准图片，第二个参数是用来比较的其他图片所在的目录，返回结果是两张图片之间不相同的数据位数量（汉明距离）。

这种算法的优点是简单快速，不受图片大小缩放的影响，缺点是图片的内容不能变更。如果在图片上加几个文字，它就认不出来了。所以，它的最佳用途是根据缩略图，找出原图。

实际应用中，往往采用更强大的pHash算法和SIFT算法，它们能够识别图片的变形。只要变形程度不超过25%，它们就能匹配原图。这些算法虽然更复杂，但是原理与上面的简便算法是一样的，就是先将图片转化成Hash字符串，然后再进行比较。

[相似图片搜索的原理（二）](https://www.ruanyifeng.com/blog/2013/03/similar_image_search_part_ii.html)

一、颜色分布法

每张图片都可以生成颜色分布的直方图（color histogram）。如果两张图片的直方图很接近，就可以认为它们很相似。

![image](https://user-images.githubusercontent.com/101693157/158818957-b0cdf2ef-678d-4164-93c2-6bdfb734a984.png)

任何一种颜色都是由红绿蓝三原色（RGB）构成的，所以上图共有4张直方图（三原色直方图 + 最后合成的直方图）。
如果每种原色都可以取256个值，那么整个颜色空间共有1600万种颜色（256的三次方）。针对这1600万种颜色比较直方图，计算量实在太大了，因此需要采用简化方法。可以将0～255分成四个区：0～63为第0区，64～127为第1区，128～191为第2区，192～255为第3区。这意味着红绿蓝分别有4个区，总共可以构成64种组合（4的3次方）。
任何一种颜色必然属于这64种组合中的一种，这样就可以统计每一种组合包含的像素数量。

![image](https://user-images.githubusercontent.com/101693157/158819069-d098c98d-92cb-40fe-9f5b-0fbce4a0001f.png)

上图是某张图片的颜色分布表，将表中最后一栏提取出来，组成一个64维向量(7414, 230, 0, 0, 8, ..., 109, 0, 0, 3415, 53929)。`这个向量就是这张图片的特征值或者叫"指纹"。`
于是，寻找相似图片就变成了找出与其最相似的向量。这可以用皮尔逊相关系数或者余弦相似度算出。

二、内容特征法

除了颜色构成，还可以从比较图片内容的相似性入手。

首先，将原图转成一张较小的灰度图片，假定为50x50像素。然后，确定一个阈值，将灰度图片转成黑白图片。

![image](https://user-images.githubusercontent.com/101693157/158819336-877bb776-3fec-4ebd-b01b-20e7619ac3d5.png)

如果两张图片很相似，它们的黑白轮廓应该是相近的。于是，问题就变成了，第一步如何确定一个合理的阈值，正确呈现照片中的轮廓？
显然，前景色与背景色反差越大，轮廓就越明显。这意味着，如果我们找到一个值，可以使得前景色和背景色各自的"类内差异最小"（minimizing the intra-class variance），或者"类间差异最大"（maximizing the inter-class variance），那么这个值就是理想的阈值。
1979年，日本学者大津展之证明了，"类内差异最小"与"类间差异最大"是同一件事，即对应同一个阈值。他提出一种简单的算法，可以求出这个阈值，这被称为"大津法"（Otsu's method）。下面就是他的计算方法。
假定一张图片共有n个像素，其中灰度值小于阈值的像素为 n1 个，大于等于阈值的像素为 n2 个（ n1 + n2 = n ）。w1 和 w2 表示这两种像素各自的比重。
　　w1 = n1 / n
　　w2 = n2 / n
再假定，所有灰度值小于阈值的像素的平均值和方差分别为 μ1 和 σ1，所有灰度值大于等于阈值的像素的平均值和方差分别为 μ2 和 σ2。于是，可以得到
　　类内差异 = w1(σ1的平方) + w2(σ2的平方)
　　类间差异 = w1w2(μ1-μ2)^2
可以证明，这两个式子是等价的：得到"类内差异"的最小值，等同于得到"类间差异"的最大值。不过，从计算难度看，后者的计算要容易一些。

![image](https://user-images.githubusercontent.com/101693157/158819356-8679505c-c7aa-4731-8ca5-4771f422f439.png)

有了50x50像素的黑白缩略图，就等于有了一个50x50的0-1矩阵。矩阵的每个值对应原图的一个像素，0表示黑色，1表示白色。这个矩阵就是一张图片的特征矩阵。

两个特征矩阵的不同之处越少，就代表两张图片越相似。这可以用"异或运算"实现（即两个值之中只有一个为1，则运算结果为1，否则运算结果为0）。对不同图片的特征矩阵进行"异或运算"，结果中的1越少，就是越相似的图片。
  
### [如何识别图像边缘？](https://www.ruanyifeng.com/blog/2016/07/edge-recognition.html)

图像识别（image recognition）是现在的热门技术。
文字识别、车牌识别、人脸识别都是它的应用。但是，这些都算初级应用，现在的技术已经发展到了这样一种地步：计算机可以识别出，这是一张狗的照片，那是一张猫的照片。

这是怎么做到的？

让我们从人眼说起，学者发现，人的视觉细胞对物体的边缘特别敏感。也就是说，我们先看到物体的轮廓，然后才判断这到底是什么东西。
  
计算机科学家受到启发，第一步也是先识别图像的边缘。

![](https://www.ruanyifeng.com/blogimg/asset/2016/bg2016072208.png)
  
加州大学的学生 Adit Deshpande 写了一篇文章[《A Beginner's Guide To Understanding Convolutional Neural Networks》](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/)，介绍了一种最简单的算法，非常具有启发性，体现了图像识别的基本思路。

![](https://www.ruanyifeng.com/blogimg/asset/2016/bg2016072203.png)
  
首先，我们要明白，`人看到的是图像，计算机看到的是一个数字矩阵。所谓"图像识别"，就是从一大堆数字中找出规律。`
  
怎样将图像转为数字呢？一般来说，为了过滤掉干扰信息，可以把图像缩小（比如缩小到 49 x 49 像素），并且把每个像素点的色彩信息转为灰度值，这样就得到了一个 49 x 49 的矩阵。
然后，从左上角开始，依次取出一个小区块，进行计算。

![image](https://user-images.githubusercontent.com/101693157/158825889-204a8385-6e0e-49f3-89b7-dbe0e1ef269d.png)

上图是取出一个 5 x 5 的区块。下面的计算以 7 x 7 的区块为例。
接着，需要有一些现成的边缘模式，比如垂直、直角、圆、锐角等等。

![](https://www.ruanyifeng.com/blogimg/asset/2016/bg2016072205.png)

上图右边是一个圆角模式，左边是它对应的 7 x 7 灰度矩阵。可以看到，圆角所在的边缘灰度值比较高，其他地方都是0。
现在，就可以进行边缘识别了。下面是一张卡通老鼠的图片。
  
![](https://www.ruanyifeng.com/blogimg/asset/2016/bg2016072206.png)
取出左上角的区块。

![](https://www.ruanyifeng.com/blogimg/asset/2016/bg2016072209.png)
取样矩阵与模式矩阵对应位置的值相乘，进行累加，得到6600。这个值相当大，它说明什么呢？

![](https://www.ruanyifeng.com/blogimg/asset/2016/bg2016072207.png)
取样矩阵移到老鼠头部，与模式矩阵相乘，得到的值是0。

乘积越大就说明越匹配，可以断定区块里的图像形状是圆角。通常会预置几十种模式，每个区块计算出最匹配的模式，然后再对整张图进行判断。

## 参考

1. [正态分布为什么常见？](https://www.ruanyifeng.com/blog/2017/08/normal-distribution.html)
2. [高斯模糊的算法](https://www.ruanyifeng.com/blog/2012/11/gaussian_blur.html)

![Alt](https://repobeats.axiom.co/api/embed/86211ab883763ed6c75fd14571647e7febb6919f.svg "Repobeats analytics image")
